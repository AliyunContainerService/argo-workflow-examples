apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  name: tcm-deepseek-finetune-with-argo
  namespace: default
spec:
  entrypoint: main
  templates:
    - name: main
      steps:
        - - name: download-dataset
            template: download-dataset
            arguments:
              parameters:
                - name: dataset-path
                  value: '{{workflow.parameters.dataset-path}}'
          - name: download-model
            template: download-model
            arguments:
              parameters:
                - name: base-model
                  value: '{{workflow.parameters.base-model}}'
                - name: model-name
                  value: '{{workflow.parameters.model-name}}'
        - - name: format-prompts
            template: format-prompts
            arguments:
              parameters:
                - name: dataset-path
                  value: '{{workflow.parameters.dataset-path}}'
                - name: format-path
                  value: '{{workflow.parameters.format-path}}'
                - name: base-model
                  value: '{{workflow.parameters.base-model}}'
        - - name: training
            template: training
            arguments:
              parameters:
                - name: format-path
                  value: '{{workflow.parameters.format-path}}'
                - name: model-path
                  value: '{{workflow.parameters.base-model}}'
                - name: output-path
                  value: '{{workflow.parameters.output-model}}'
        - - name: inference-finetuned
            template: inference-template
            arguments:
              parameters:
                - name: model-path
                  value: '{{workflow.parameters.output-model}}'
          - name: inference-basemodel
            template: inference-template
            arguments:
              parameters:
                - name: model-path
                  value: '{{workflow.parameters.base-model}}'
    - name: download-dataset
      inputs:
        parameters:
          - name: dataset-path
            default: /mnt/vol/datasets
      script:
        image: acr-multiple-clusters-registry.cn-hangzhou.cr.aliyuncs.com/serverless-argo/deepseek-finetune:v4
        source: |-
          import os
          import sys
          sys.path.append(os.getcwd())
          from datasets import load_dataset
          import os
          save_path = '{{inputs.parameters.dataset-path}}'
          print('Downloading dataset...')
          if not os.path.exists(save_path):
              dataset = load_dataset('SylvanL/Traditional-Chinese-Medicine-Dataset-SFT', split='train')
              dataset.save_to_disk(save_path)
          print(f'Dataset saved to {save_path}')
        command:
          - python
        volumeMounts:
          - name: workdir
            mountPath: /mnt/vol
    - name: download-model
      inputs:
        parameters:
          - name: base-model
            default: /mnt/vol/model
          - name: model-name
            default: model-name
      script:
        image: acr-multiple-clusters-registry.cn-hangzhou.cr.aliyuncs.com/serverless-argo/deepseek-finetune:v4
        source: |-
          import os
          import sys
          sys.path.append(os.getcwd())
          from huggingface_hub import snapshot_download
          download_path = '{{inputs.parameters.base-model}}'
          if not os.path.exists(download_path):
              snapshot_download(repo_id='unsloth/{{inputs.parameters.model-name}}', local_dir=download_path, ignore_patterns=['*.msgpack', '*.h5', '*.tflite'])
          print(f'Model downloaded to {download_path}')
        command:
          - python
        volumeMounts:
          - name: workdir
            mountPath: /mnt/vol
    - name: format-prompts
      inputs:
        parameters:
          - name: dataset-path
            default: /mnt/data/datasets
          - name: base-model
            default: /mnt/data
          - name: format-path
            value: /mnt/data/format
      script:
        image: acr-multiple-clusters-registry.cn-hangzhou.cr.aliyuncs.com/serverless-argo/deepseek-finetune:v4
        source: "import os\nimport sys\nsys.path.append(os.getcwd())\nfrom datasets\
        \ import load_from_disk\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\
        import sys\nsave_path = '{{inputs.parameters.base-model}}'\nformat_path =\
        \ '{{inputs.parameters.format-path}}'\nif os.path.exists(format_path):\n \
        \   sys.exit(0)\ntokenizer = AutoTokenizer.from_pretrained(save_path)\nEOS_TOKEN\
        \ = tokenizer.eos_token\ndataset = load_from_disk('{{workflow.parameters.dataset-path}}')\n\
        alpaca_prompt = '\u4EE5\u4E0B\u662F\u63CF\u8FF0\u4EFB\u52A1\u7684\u8BF4\u660E\
        \uFF0C\u5E76\u642D\u914D\u63D0\u4F9B\u66F4\u591A\u4E0A\u4E0B\u6587\u7684\u8F93\
        \u5165\u3002\\n    \u5199\u51FA\u9002\u5F53\u5B8C\u6210\u8BF7\u6C42\u7684\u56DE\
        \u590D\u3002\u5728\u56DE\u7B54\u4E4B\u524D\uFF0C\u8BF7\u4ED4\u7EC6\u601D\u8003\
        \u95EE\u9898\u5E76\u521B\u5EFA\u5FAA\u5E8F\u6E10\u8FDB\u7684\u601D\u8DEF\u94FE\
        \uFF0C\u4EE5\u786E\u4FDD\u505A\u51FA\u5408\u4E4E\u903B\u8F91\u4E14\u51C6\u786E\
        \u7684\u56DE\u7B54\u3002\\n\\n    ### Instruction:\\n    \u60A8\u662F\u4E00\
        \u4F4D\u5728\u4E2D\u533B\u7684\u4E34\u5E8A\u63A8\u7406\u3001\u8BCA\u65AD\u548C\
        \u6CBB\u7597\u8BA1\u5212\u7B49\u65B9\u9762\u5177\u6709\u5177\u6709\u4E30\u5BCC\
        \u7ECF\u9A8C\u7684\u533B\u5B66\u4E13\u5BB6\u3002\u8BF7\u56DE\u7B54\u4EE5\u4E0B\
        \u533B\u5B66\u95EE\u9898\u3002\\n\\n    ### Input:\\n    {}\\n\\n    ### Response:\\\
        n    {}'\n\ndef formatting_prompts_func(examples):\n    instructions = examples['instruction']\n\
        \    inputs = examples['input']\n    outputs = examples['output']\n    texts\
        \ = []\n    for (instruction, input, output) in zip(instructions, inputs,\
        \ outputs):\n        text = alpaca_prompt.format(input, output) + EOS_TOKEN\n\
        \        texts.append(text)\n    return {'text': texts}\ndataset = dataset.map(formatting_prompts_func,\
        \ batched=True)\nprint('Formatting prompts')\ndataset.save_to_disk('{{inputs.parameters.format-path}}')"
        command:
          - python
        volumeMounts:
          - name: workdir
            mountPath: /mnt/vol
        resources:
          limits:
            cpu: '16'
            memory: 32Gi
          requests:
            cpu: '16'
            memory: 32Gi
    - name: training
      inputs:
        parameters:
          - name: format-path
            default: /mnt/data/datasets
          - name: model-path
            value: Dataset download started
          - name: output-path
            value: ''
      metadata:
        annotations:
          k8s.aliyun.com/eci-gpu-driver-version: tesla=525.85.12
          k8s.aliyun.com/eci-use-specs: ecs.gn7i-c16g1.4xlarge
      script:
        image: acr-multiple-clusters-registry.cn-hangzhou.cr.aliyuncs.com/serverless-argo/deepseek-finetune:v4
        source: |-
          import os
          import sys
          sys.path.append(os.getcwd())
          from unsloth import is_bfloat16_supported
          from unsloth import FastLanguageModel
          from trl import SFTTrainer
          from transformers import TrainingArguments
          from datasets import load_from_disk
          import sys
          max_seq_length = 2048
          dataset = load_from_disk('{{inputs.parameters.format-path}}')
          base = '{{inputs.parameters.model-path}}'
          output_dir = '{{inputs.parameters.output-path}}'
          if os.path.exists(output_dir):
              sys.exit(0)
          (model, tokenizer) = FastLanguageModel.from_pretrained(model_name=base, max_seq_length=4096, local_files_only=True, dtype=None, load_in_4bit=True)
          model = FastLanguageModel.get_peft_model(model, r=16, target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'], lora_alpha=16, lora_dropout=0, bias='none', use_gradient_checkpointing='unsloth', random_state=3407, use_rslora=False, loftq_config=None)
          trainer = SFTTrainer(model=model, tokenizer=tokenizer, train_dataset=dataset, dataset_text_field='text', max_seq_length=max_seq_length, dataset_num_proc=2, args=TrainingArguments(per_device_train_batch_size=2, gradient_accumulation_steps=4, warmup_steps=5, max_steps=60, learning_rate=0.0002, fp16=not is_bfloat16_supported(), bf16=is_bfloat16_supported(), logging_steps=10, optim='adamw_8bit', weight_decay=0.01, lr_scheduler_type='linear', seed=3407, output_dir='outputs'))
          print('Fine-tuning model')
          trainer_stats = trainer.train()
          print(trainer_stats)
          save_path = '{{inputs.parameters.output-path}}'
          model.save_pretrained(save_path)
          tokenizer.save_pretrained(save_path)
        command:
          - python
        volumeMounts:
          - name: workdir
            mountPath: /mnt/vol
        resources:
          limits:
            cpu: '12'
            memory: 40Gi
            nvidia.com/gpu: '1'
          requests:
            cpu: '8'
            memory: 20Gi
            nvidia.com/gpu: '1'
    - name: inference-template
      inputs:
        parameters:
          - name: model-path
            default: /mnt/data/datasets
      metadata:
        annotations:
          k8s.aliyun.com/eci-gpu-driver-version: tesla=525.85.12
          k8s.aliyun.com/eci-use-specs: ecs.gn7i-c16g1.4xlarge
      script:
        image: acr-multiple-clusters-registry.cn-hangzhou.cr.aliyuncs.com/serverless-argo/deepseek-finetune:v4
        source: "import os\nimport sys\nsys.path.append(os.getcwd())\nfrom unsloth import\
        \ FastLanguageModel\nmodelpath = '{{inputs.parameters.model-path}}'\n(model,\
        \ tokenizer) = FastLanguageModel.from_pretrained(model_name=modelpath, max_seq_length=2048,\
        \ dtype=None, load_in_4bit=True)\nprompt_style = '\u4EE5\u4E0B\u662F\u63CF\
        \u8FF0\u4EFB\u52A1\u7684\u8BF4\u660E\uFF0C\u5E76\u642D\u914D\u63D0\u4F9B\u66F4\
        \u591A\u4E0A\u4E0B\u6587\u7684\u8F93\u5165\u3002\\n    \u5199\u51FA\u9002\u5F53\
        \u5B8C\u6210\u8BF7\u6C42\u7684\u56DE\u590D\u3002\u5728\u56DE\u7B54\u4E4B\u524D\
        \uFF0C\u8BF7\u4ED4\u7EC6\u601D\u8003\u95EE\u9898\u5E76\u521B\u5EFA\u5FAA\u5E8F\
        \u6E10\u8FDB\u7684\u601D\u8DEF\u94FE\uFF0C\u4EE5\u786E\u4FDD\u505A\u51FA\u5408\
        \u4E4E\u903B\u8F91\u4E14\u51C6\u786E\u7684\u56DE\u7B54\u3002\\n\\n    ###\
        \ Instruction:\\n    \u60A8\u662F\u4E00\u4F4D\u5728\u4E2D\u533B\u7684\u4E34\
        \u5E8A\u63A8\u7406\u3001\u8BCA\u65AD\u548C\u6CBB\u7597\u8BA1\u5212\u7B49\u65B9\
        \u9762\u5177\u6709\u5177\u6709\u4E30\u5BCC\u7ECF\u9A8C\u7684\u533B\u5B66\u4E13\
        \u5BB6\u3002\u8BF7\u56DE\u7B54\u4EE5\u4E0B\u533B\u5B66\u95EE\u9898\u3002\\\
        n\\n    ### Question:\\n    {}\\n\\n    ### Response:\\n    {}'\nquestion\
        \ = '\u4E45\u54B3\u4E0D\u6B62\u600E\u4E48\u529E\uFF1F'\nFastLanguageModel.for_inference(model)\n\
        inputs = tokenizer([prompt_style.format(question, '')], return_tensors='pt').to('cuda')\n\
        outputs = model.generate(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask,\
        \ max_new_tokens=1200, use_cache=True)\nresponse = tokenizer.batch_decode(outputs)\n\
        outputs = response[0].split('### Response:')[1]\nif 'think' in outputs:\n\
        \    outputs = outputs.split('think>')[1]\nprint(outputs)\nwith open('/tmp/response.txt',\
        \ 'w', encoding='utf-8') as f:\n    f.write(outputs)"
        command:
          - python
        volumeMounts:
          - name: workdir
            mountPath: /mnt/vol
        resources:
          limits:
            cpu: '12'
            memory: 40Gi
            nvidia.com/gpu: '1'
          requests:
            cpu: '8'
            memory: 20Gi
            nvidia.com/gpu: '1'
  volumes:
    - name: workdir
      persistentVolumeClaim:
        claimName: pvc-oss
  arguments:
    parameters:
      - name: dataset-path
        value: /mnt/vol/traditional-chinese-medicine-data
      - name: format-path
        value: /mnt/vol/traditional-chinese-medicine-fromat-data
      - name: base-model
        value: /mnt/vol/deepseek-basemodel
      - name: output-model
        value: /mnt/vol/deepseek-finetuned
      - name: model-name
        value: DeepSeek-R1-Distill-Qwen-7B
